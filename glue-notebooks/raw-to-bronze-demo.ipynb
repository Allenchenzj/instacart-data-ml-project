{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw to Bronze Demo Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%idle_timeout 60\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 2\n",
    "%additional_python_modules \n",
    "%extra_jars \n",
    "%%configure\n",
    "{\n",
    "    \"--datalake-formats\": \"iceberg\",\n",
    "    \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO --conf spark.sql.catalog.glue_catalog.warehouse=s3://instacart-data-eng-project/iceberg-warehouse/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Initialization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"instacart-data-eng-project\"\n",
    "CATALOG_NAME = \"glue_catalog\"\n",
    "DATABASE_NAME = \"bronze\"\n",
    "TABLE_NAME = \"orders\"\n",
    "raw_path = f\"s3://{BUCKET_NAME}/raw/{TABLE_NAME}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Glue database (Bronze & Silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {CATALOG_NAME}.{DATABASE_NAME} LOCATION 's3://{BUCKET_NAME}/{DATABASE_NAME}/'\")\n",
    "spark.sql(f\"SHOW DATABASES IN {CATALOG_NAME}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(raw_path)\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_count = df.count()\n",
    "print(f\"Read {record_count} records from orders table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add audit columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"ingest_timestamp\", F.current_timestamp()) \\\n",
    "    .withColumn(\"source_file\", F.input_file_name())\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write the raw data to bronze iceberg table (full refresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table_name = f\"{CATALOG_NAME}.{DATABASE_NAME}.{TABLE_NAME}\"\n",
    "table_location = f\"s3://{BUCKET_NAME}/{DATABASE_NAME}/{TABLE_NAME}\"\n",
    "    \n",
    "df.writeTo(full_table_name) \\\n",
    "    .option(\"check-ordering\", \"false\") \\\n",
    "    .tableProperty(\"format-version\", \"2\") \\\n",
    "    .tableProperty(\"location\", table_location) \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"Data written to Iceberg table {full_table_name} at location {table_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(f\"{CATALOG_NAME}.{DATABASE_NAME}.{TABLE_NAME}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {CATALOG_NAME}.{DATABASE_NAME}.{TABLE_NAME}\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write the raw data to bronze iceberg table (incremental load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orders = [\n",
    "    (100, 11111, \"prior\", 1, 3, 18, 2.0),\n",
    "    (101, 22222, \"prior\", 2, 4, 19, 5.0),\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"eval_set\", StringType(), True),\n",
    "    StructField(\"order_number\", IntegerType(), True),\n",
    "    StructField(\"order_dow\", IntegerType(), True),\n",
    "    StructField(\"order_hour_of_day\", IntegerType(), True),\n",
    "    StructField(\"days_since_prior_order\", FloatType(), True),\n",
    "])\n",
    "\n",
    "new_orders_df = spark.createDataFrame(new_orders, orders_schema)\n",
    "\n",
    "new_orders_df.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(f\"{raw_path}/demo/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{raw_path}/demo/\")\n",
    "df_new.show(5)\n",
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT COUNT(*) AS total_records FROM {full_table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table_name = f\"{CATALOG_NAME}.{DATABASE_NAME}.{TABLE_NAME}\"\n",
    "    \n",
    "df_new.withColumn(\"ingest_timestamp\", F.current_timestamp()) \\\n",
    "    .withColumn(\"source_file\", F.input_file_name()) \\\n",
    "    .writeTo(full_table_name) \\\n",
    "    .option(\"check-ordering\", \"false\") \\\n",
    "    .append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT COUNT(*) AS total_records FROM {full_table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(f\"{CATALOG_NAME}.{DATABASE_NAME}.{TABLE_NAME}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean up table and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG_NAME}.{DATABASE_NAME}.{TABLE_NAME}\")\n",
    "spark.sql(f\"DROP DATABASE IF EXISTS {CATALOG_NAME}.{DATABASE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Iceberg Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {CATALOG_NAME}.{DATABASE_NAME}.{TABLE_NAME}.history\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orders = [\n",
    "    (100, 11111, \"prior\", 1, 3, 18, 2.0),\n",
    "    (101, 22222, \"prior\", 2, 4, 19, 5.0),\n",
    "]\n",
    "\n",
    "new_orders_df = spark.createDataFrame(new_orders, orders_schema)\n",
    "new_orders_df.writeTo(f\"{CATALOG_NAME}.{DATABASE_NAME}.orders_bronze\").append()\n",
    "\n",
    "print(\"新数据已追加！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT COUNT(1) FROM {full_table_name} TIMESTAMP AS OF '2026-01-28 08:49:00'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT COUNT(1) FROM {full_table_name} TIMESTAMP AS OF '2026-01-28 09:27:00'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_raw(table_name):\n",
    "    \"\"\"\n",
    "    read CSV files from S3 raw layer\n",
    "    param table_name: table name (corresponding to folder name in raw layer)\n",
    "    return: DataFrame\n",
    "    \"\"\"\n",
    "    path = f\"s3://{BUCKET_NAME}/raw/{table_name}/\"\n",
    "    \n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(path) \\\n",
    "        .withColumn(\"ingest_timestamp\", F.current_timestamp()) \\\n",
    "        .withColumn(\"source_file\", F.input_file_name())\n",
    "    \n",
    "    print(f\"Read {df.count()} records from {path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_bronze_iceberg_full(df, table_name):\n",
    "    \"\"\"\n",
    "    Write DataFrame to Bronze Iceberg table\n",
    "    param df: DataFrame to write\n",
    "    param table_name: table name\n",
    "    \"\"\"\n",
    "    full_table_name = f\"{CATALOG_NAME}.{DATABASE_NAME}.{table_name}\"\n",
    "    table_location = f\"s3://{BUCKET_NAME}/bronze/{table_name}\"\n",
    "    \n",
    "    df.writeTo(full_table_name) \\\n",
    "        .tableProperty(\"format-version\", \"2\") \\\n",
    "        .tableProperty(\"location\", table_location) \\\n",
    "        .createOrReplace()\n",
    "    \n",
    "    print(f\"✅ Successfully wrote to {full_table_name}\")\n",
    "    print(f\"   Location: {table_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_bronze_iceberg_incremental(spark, df, table_name):\n",
    "    \"\"\"Write DataFrame to Iceberg table (incremental - append).\"\"\"\n",
    "    full_table_name = f\"{CATALOG_NAME}.{DATABASE_NAME}.{table_name}\"\n",
    "    \n",
    "    df.writeTo(full_table_name) \\\n",
    "        .option(\"check-ordering\", \"false\") \\\n",
    "        .append()\n",
    "    \n",
    "    print(f\"✅ Successfully appended to {full_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\"orders\", \"products\", \"aisles\", \"departments\", \"order_products\"]\n",
    "\n",
    "for table in tables:\n",
    "    df = read_csv_from_raw(table)\n",
    "    write_to_bronze_iceberg_full(df, table)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
