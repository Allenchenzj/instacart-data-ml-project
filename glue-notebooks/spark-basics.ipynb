{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PySpark Basics Demo\n",
                "\n",
                "This notebook demonstrates basic PySpark operations and lays the groundwork for later ETL development.\n",
                "\n",
                "**Runtime**: AWS Glue Interactive Session (Notebook)\n",
                "\n",
                "**Learning objectives**:\n",
                "- Understand SparkSession initialization\n",
                "- Learn basic DataFrame operations\n",
                "- Read and write CSV and Parquet files"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize Glue Session\n",
                "\n",
                "In a Glue Notebook, configure the magic commands first to initialize the environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Glue Notebook magic configuration\n",
                "%idle_timeout 60\n",
                "%glue_version 4.0\n",
                "%worker_type G.1X\n",
                "%number_of_workers 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "from pyspark.context import SparkContext\n",
                "from awsglue.context import GlueContext\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.sql.types import *\n",
                "\n",
                "# Initialize Glue Context\n",
                "sc = SparkContext.getOrCreate()\n",
                "glueContext = GlueContext(sc)\n",
                "spark = glueContext.spark_session\n",
                "\n",
                "print(\"Spark version:\", spark.version)\n",
                "print(\"Initialization complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Create sample data\n",
                "\n",
                "Let's create some sample data to simulate an e-commerce scenario."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create product data\n",
                "products_data = [\n",
                "    (1, \"Organic Banana\", 24, 4),\n",
                "    (2, \"Whole Milk\", 84, 16),\n",
                "    (3, \"Organic Strawberries\", 24, 4),\n",
                "    (4, \"Bag of Organic Bananas\", 24, 4),\n",
                "    (5, \"Organic Baby Spinach\", 123, 4),\n",
                "    (6, \"Large Lemon\", 24, 4),\n",
                "    (7, \"Strawberries\", 24, 4),\n",
                "    (8, \"Limes\", 24, 4),\n",
                "    (9, \"Organic Avocado\", 24, 4),\n",
                "    (10, \"Organic Whole Milk\", 84, 16),\n",
                "]\n",
                "\n",
                "products_schema = StructType([\n",
                "    StructField(\"product_id\", IntegerType(), False),\n",
                "    StructField(\"product_name\", StringType(), True),\n",
                "    StructField(\"aisle_id\", IntegerType(), True),\n",
                "    StructField(\"department_id\", IntegerType(), True),\n",
                "])\n",
                "\n",
                "products_df = spark.createDataFrame(products_data, products_schema)\n",
                "products_df.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create aisle data\n",
                "aisles_data = [\n",
                "    (24, \"fresh fruits\"),\n",
                "    (84, \"milk\"),\n",
                "    (123, \"packaged vegetables fruits\"),\n",
                "]\n",
                "\n",
                "aisles_df = spark.createDataFrame(aisles_data, [\"aisle_id\", \"aisle\"])\n",
                "aisles_df.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create department data\n",
                "departments_data = [\n",
                "    (4, \"produce\"),\n",
                "    (16, \"dairy eggs\"),\n",
                "]\n",
                "\n",
                "departments_df = spark.createDataFrame(departments_data, [\"department_id\", \"department\"])\n",
                "departments_df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Basic DataFrame operations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View schema\n",
                "print(\"=== Products Schema ===\")\n",
                "products_df.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count records\n",
                "print(f\"Product count: {products_df.count()}\")\n",
                "print(f\"Aisle count: {aisles_df.count()}\")\n",
                "print(f\"Department count: {departments_df.count()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select specific columns\n",
                "products_df.select(\"product_id\", \"product_name\").show(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter data\n",
                "# Find all organic products\n",
                "organic_products = products_df.filter(\n",
                "    F.col(\"product_name\").contains(\"Organic\")\n",
                ")\n",
                "organic_products.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add a new column\n",
                "products_with_flag = products_df.withColumn(\n",
                "    \"is_organic\",\n",
                "    F.when(F.col(\"product_name\").contains(\"Organic\"), True).otherwise(False)\n",
                ")\n",
                "products_with_flag.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. JOIN operations\n",
                "\n",
                "This is the core operation for Bronze to Silver transformations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LEFT JOIN: products + aisles\n",
                "products_with_aisle = products_df.join(\n",
                "    aisles_df,\n",
                "    on=\"aisle_id\",\n",
                "    how=\"left\"\n",
                ")\n",
                "products_with_aisle.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Multi-table JOIN: products + aisles + departments\n",
                "dim_products = products_df \\\n",
                "    .join(aisles_df, \"aisle_id\", \"left\") \\\n",
                "    .join(departments_df, \"department_id\", \"left\") \\\n",
                "    .select(\n",
                "        F.col(\"product_id\"),\n",
                "        F.col(\"product_name\"),\n",
                "        F.col(\"aisle_id\"),\n",
                "        F.col(\"aisle\"),\n",
                "        F.col(\"department_id\"),\n",
                "        F.col(\"department\")\n",
                "    )\n",
                "\n",
                "print(\"=== Dimension table: dim_products ===\")\n",
                "dim_products.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Aggregations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count products by department\n",
                "products_by_dept = dim_products.groupBy(\"department\").agg(\n",
                "    F.count(\"product_id\").alias(\"product_count\")\n",
                ")\n",
                "products_by_dept.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count products by aisle and sort\n",
                "products_by_aisle = dim_products.groupBy(\"aisle\").agg(\n",
                "    F.count(\"product_id\").alias(\"product_count\")\n",
                ").orderBy(F.desc(\"product_count\"))\n",
                "\n",
                "products_by_aisle.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. De-duplication and null handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create data with duplicates and nulls\n",
                "dirty_data = [\n",
                "    (1, \"Apple\", 10),\n",
                "    (1, \"Apple\", 10),      # duplicate\n",
                "    (2, \"Banana\", None),   # null\n",
                "    (3, None, 20),         # null\n",
                "]\n",
                "\n",
                "dirty_df = spark.createDataFrame(dirty_data, [\"id\", \"name\", \"price\"])\n",
                "print(\"=== Raw dirty data ===\")\n",
                "dirty_df.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Remove duplicates\n",
                "deduped_df = dirty_df.dropDuplicates([\"id\"])\n",
                "print(\"=== After de-duplication ===\")\n",
                "deduped_df.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fill nulls\n",
                "clean_df = deduped_df.fillna({\n",
                "    \"name\": \"Unknown\",\n",
                "    \"price\": 0\n",
                "})\n",
                "print(\"=== After filling nulls ===\")\n",
                "clean_df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Write files\n",
                "\n",
                "**Note**: Replace `YOUR_BUCKET_NAME` with your S3 bucket name."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your S3 bucket name\n",
                "BUCKET_NAME = \"YOUR_BUCKET_NAME\"  # ‚Üê replace with your bucket name\n",
                "\n",
                "# Write CSV\n",
                "dim_products.write \\\n",
                "    .mode(\"overwrite\") \\\n",
                "    .option(\"header\", \"true\") \\\n",
                "    .csv(f\"s3://{BUCKET_NAME}/demo/dim_products_csv/\")\n",
                "\n",
                "print(\"CSV write complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Write Parquet (recommended format)\n",
                "dim_products.write \\\n",
                "    .mode(\"overwrite\") \\\n",
                "    .parquet(f\"s3://{BUCKET_NAME}/demo/dim_products_parquet/\")\n",
                "\n",
                "print(\"Parquet write complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read back the Parquet we just wrote\n",
                "read_back_df = spark.read.parquet(f\"s3://{BUCKET_NAME}/demo/dim_products_parquet/\")\n",
                "read_back_df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Class exercise\n",
                "\n",
                "Try to complete the following tasks:\n",
                "\n",
                "1. Create an orders DataFrame with order_id, user_id, order_dow (day of week)\n",
                "2. Count the number of orders per day of week\n",
                "3. Find the day of week with the most orders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise code area\n",
                "# Hint:\n",
                "# orders_data = [(1, 101, 0), (2, 102, 1), ...]\n",
                "# orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"user_id\", \"order_dow\"])\n",
                "\n",
                "# Your code:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Clean up resources\n",
                "\n",
                "Remember to stop the session to save cost."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stop Spark Session\n",
                "# spark.stop()\n",
                "# print(\"Session stopped\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Glue PySpark",
            "language": "python",
            "name": "glue_pyspark"
        },
        "language_info": {
            "name": "python",
            "version": "3.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
